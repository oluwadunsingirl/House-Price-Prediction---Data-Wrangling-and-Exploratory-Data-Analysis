#split data 80% for training and 20% for testing the model
X = df.drop('price', axis=1)
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

#Using a baseline model building
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print('r2 score:', r2_score(y_test, y_pred))
print('mse:', mean_squared_error(y_test, y_pred))

## r2 score = 0.6529242642153185
mse: 1754318687330.6633
- An r2 score of 0.6529242642153185 meaning that, the baseline model linear regression explains 65% of the
varaince in price.
- the mse shows that the predictions still have large error (in millions), meaning the linear model struggles 
with capturing complex patterns.

# Using other models for a better r2 score
from sklearn.tree import DecisionTreeRegressor

# Train Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Predict on the test set
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
print('Decision Tree R2 Score:', r2_score(y_test, y_pred_dt))
print('Decision Tree MSE:', mean_squared_error(y_test, y_pred_dt))

Result:
Decision Tree R2 Score: 0.4771459275854347
Decision Tree MSE: 2642802637614.6787

# Using the Random Forest Model
from sklearn.ensemble import RandomForestRegressor

# Train Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print('Random Forest R2 Score:', r2_score(y_test, y_pred_rf))
print('Random Forest MSE:', mean_squared_error(y_test, y_pred_rf))

Result:
Random Forest R2 Score: 0.6114024924156645
Random Forest MSE: 1964193399645.3335

#comparing the r2 score of the models used;

models = ['Linear Regression', 'Decision Tree', 'Random Forest']
r2_scores = [r2_score(y_test, y_pred), r2_score(y_test, y_pred_dt), r2_score(y_test, y_pred_rf)]

plt.figure(figsize=(8, 6))
sns.barplot(x=models, y=r2_scores)
plt.ylabel('R2 Score')
plt.title('Model Comparison by R2 Score')
plt.ylim(0, 1)
plt.show()

#The decision tree had the lowest r2 score
